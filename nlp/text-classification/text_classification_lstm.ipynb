{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5907e216",
   "metadata": {},
   "source": [
    "# Text Classification with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62da8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanpua/opt/anaconda3/envs/nlp-pytorch-legacy/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Text processing\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3384b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\"\n",
    "      \"\\n\".format(str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd2edc",
   "metadata": {},
   "source": [
    "# EDA of Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000aefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"all-data.csv\"\n",
    "\n",
    "df = pd.read_csv(raw_data_path, encoding = \"ISO-8859-1\", names=['sentiment', \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac07bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                               text\n",
       "0      neutral  According to Gran , the company has no plans t...\n",
       "1      neutral  Technopolis plans to develop in stages an area...\n",
       "2     negative  The international electronic industry company ...\n",
       "3     positive  With the new production plant the company woul...\n",
       "4     positive  According to the company 's updated strategy f...\n",
       "...        ...                                                ...\n",
       "4841  negative  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844  negative  Net sales of the Paper segment decreased to EU...\n",
       "4845  negative  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b3df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of news: 4846\n",
      "----------------------------------------\n",
      "Split by category:\n",
      "neutral     2879\n",
      "positive    1363\n",
      "negative     604\n",
      "Name: sentiment, dtype: int64\n",
      "----------------------------------------\n",
      "Number of categories: 3\n"
     ]
    }
   ],
   "source": [
    "print('Total number of news: {}'.format(len(df)))\n",
    "print(40*'-')\n",
    "print('Split by category:')\n",
    "print(df[\"sentiment\"].value_counts())\n",
    "print(40*'-')\n",
    "nr_categories = len(df[\"sentiment\"].unique())\n",
    "print(\"Number of categories: {n}\".format(n=nr_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89fc251",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcefb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with empty text\n",
    "df.drop( df[df.text.str.len() < 5].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ea65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Label Columns\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['sentiment'] = le.fit_transform(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64fe374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim text to first_n_words\n",
    "\n",
    "first_n_words = 200\n",
    "\n",
    "def trim_string(x):\n",
    "    x = x.split(maxsplit=first_n_words)\n",
    "    x = ' '.join(x[:first_n_words])\n",
    "    return x\n",
    "\n",
    "df['text'] = df['text'].apply(trim_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a7a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all-data-v1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c2707",
   "metadata": {},
   "source": [
    "# Load Data into DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56f8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import spacy\n",
    "spacy_tokeniser = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351b8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ivanpua/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # Download stopwords from NLTK  \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english') # from NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b929226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_field = Field(sequential=False, \n",
    "                    use_vocab=False, \n",
    "                    batch_first=True)\n",
    "text_field = Field(tokenize=spacy_tokeniser, \n",
    "                   lower=False, \n",
    "                   include_lengths=True, \n",
    "                   batch_first=True,\n",
    "                   stop_words=stopWords\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "390d7b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 s, sys: 128 ms, total: 25.7 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fields = [('sentiment', label_field), ('text', text_field)]\n",
    "\n",
    "\n",
    "dataset = TabularDataset('all-data-v1.csv', \n",
    "                         format='csv',\n",
    "                         fields=fields,\n",
    "                         skip_header=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7492c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = dataset.split(split_ratio=0.8)\n",
    "\n",
    "trainLoader, valLoader = BucketIterator.splits((train, validate),\n",
    "                                                shuffle=True,\n",
    "                                                batch_size=32,\n",
    "                                                sort_key=lambda x: len(x.text),\n",
    "                                                sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 653M/862M [05:35<01:54, 1.83MB/s]"
     ]
    }
   ],
   "source": [
    "# Or use a pre-trained embeddings e.g. Glove, Fasttext\n",
    "from torchtext.vocab import GloVe\n",
    "word_len = 50\n",
    "wordVectors = GloVe(name='6B', dim=word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c04afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(train, vectors=wordVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945836a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all-data-v1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab108196",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7aa1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension=128):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(len(text_field.vocab), 50)\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=50,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, 3)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "#         text_emb = self.embedding(text) used when you don't want to have pre-trained weights\n",
    "\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "#         packed_output, _ = self.lstm(packed_input)\n",
    "#         output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "#         out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "#         out_reverse = output[:, 0, self.dimension:]\n",
    "#         out_reduced = torch.cat((out_forward, out_reverse), dim=1)\n",
    "        x, (hidden, cn) = self.lstm(packed_input)\n",
    "\n",
    "        \"\"\"\n",
    "        The forward network of the last LSTM layer (hidden[-1, :, :]) \n",
    "        contains information about previous inputs, \n",
    "        whereas the backward network (hidden[-2, :, :]) \n",
    "        contains information about following inputs.\n",
    "        We take the last hidden state of the forward output and \n",
    "        the last hidden state of the backward output and merge them together.\n",
    "        \"\"\"\n",
    "        \n",
    "        out_reduced = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "#         text_fea = torch.squeeze(text_fea, 1) Only for binary labels\n",
    "        text_out = self.relu(text_fea)\n",
    "\n",
    "        return text_out\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b0887",
   "metadata": {},
   "source": [
    "# Loss Function (off the shelf or custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for creating the loss function.  The labels and outputs from your\n",
    "    network will be passed to the forward method during training.\n",
    "    https://discuss.pytorch.org/t/how-to-combine-multiple-criterions-to-a-loss-function/348/7\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(loss, self).__init__()\n",
    "\n",
    "    def forward(self, categoryOutput, categoryTarget):\n",
    "        \n",
    "        # Do not round to 0 or 1 here, only convert in the convertNetOutput method for Accuracy\n",
    "        # rating_loss = F.binary_cross_entropy(ratingOutput.float(), ratingTarget.float())\n",
    "        cat_loss = F.cross_entropy(categoryOutput.float(), categoryTarget.float())\n",
    "        \n",
    "        # total_loss = rating_loss + cat_loss\n",
    "        \n",
    "        return cat_loss\n",
    "\n",
    "\n",
    "lossFunc = loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84249dd7",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394aff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as toptim\n",
    "model = LSTM()\n",
    "model = model.to(device)\n",
    "# epochs = 50\n",
    "optimizer = toptim.Adam(model.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371bdf1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.CrossEntropyLoss(), # or custom from above\n",
    "          train_loader = trainLoader,\n",
    "          valid_loader = valLoader,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(trainLoader) // 2,\n",
    "          file_path = \"./output\",\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for (labels, (text, text_len)), _ in train_loader: \n",
    "                        \n",
    "            labels = labels.to(device)\n",
    "            text = textField.vocab.vectors[text].to(device)\n",
    "            text_len = text_len.to(device)\n",
    "            output = model(text, text_len)\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                  # validation loop\n",
    "                    for (labels, (text, text_len)), _ in valid_loader:\n",
    "                        labels = labels.to(device)\n",
    "                        text = textField.vocab.vectors[text].to(device)\n",
    "                        text_len = text_len.to(device)\n",
    "                        output = model(text, text_len)\n",
    "                        \n",
    "\n",
    "                        loss = criterion(output, labels)\n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "                    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')\n",
    "\n",
    "\n",
    "\n",
    "train(model=model, optimizer=optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = \"./output\"\n",
    "\n",
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (labels, (text, text_len)), _ in test_loader:           \n",
    "            labels = labels.to(device)\n",
    "            text = text.to(device)\n",
    "            text_len = text_len.to(device)\n",
    "            output = model(text, text_len)\n",
    "            \n",
    "\n",
    "#             output = torch.round(output) # for binary labels\n",
    "            \n",
    "            # obtain index with the maximum probability, representing the predicted category \n",
    "            _, output = torch.max(output.data, 1)\n",
    "            y_pred.extend(output.long().tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "    \n",
    "    print(y_pred)\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=['Negative', 'Neutral', 'Positive'], digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=['Negative', 'Neutral', 'Positive'])\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    ax.yaxis.set_ticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    \n",
    "    \n",
    "best_model = LSTM().to(device)\n",
    "optimizer = toptim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model, optimizer)\n",
    "evaluate(best_model, valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487cf33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-torch-legacy",
   "language": "python",
   "name": "nlp-torch-legacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
